{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "import contractions\n",
    "import re \n",
    "import string \n",
    "import unicodedata\n",
    "import joblib\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNoise(text):\n",
    "    text = text.lower()\n",
    "    parser = BeautifulSoup(text, \"html.parser\")\n",
    "    text = parser.get_text()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def removeNonaASCII(words):\n",
    "    newWords = []\n",
    "    for word in words:\n",
    "        newWord = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        newWords.append(newWord)\n",
    "    return newWords\n",
    "\n",
    "def toLowercase(words):\n",
    "    newWords = []\n",
    "    for word in words:\n",
    "        newWord = word.lower()\n",
    "        newWords.append(newWord)\n",
    "    return newWords\n",
    "\n",
    "def removePunctuation(words):\n",
    "    newWords = []\n",
    "    for word in words:\n",
    "        newWord = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if newWord != '':\n",
    "            newWords.append(newWord)\n",
    "    return newWords\n",
    "\n",
    "def replaceNumbers(words):\n",
    "    engine = inflect.engine()\n",
    "    newWords = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            newWord = engine.number_to_words(word)\n",
    "            newWords.append(newWord)\n",
    "        else:\n",
    "            newWords.append(word)\n",
    "    return newWords\n",
    "\n",
    "def removeStopwords(words):\n",
    "    newWords = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            newWords.append(word)\n",
    "    return newWords\n",
    "\n",
    "def stemWords(words):\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatizeVerbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalizeText(words):\n",
    "    words = removeNonaASCII(words)\n",
    "    words = toLowercase(words)\n",
    "    words = removePunctuation(words)\n",
    "    words = removeStopwords(words)\n",
    "    words = stemWords(words)\n",
    "    words = lemmatizeVerbs(words)\n",
    "    return words\n",
    "\n",
    "def processText(text):\n",
    "    text = removeNoise(text)\n",
    "    text = ' '.join(normalizeText(tokenize(text)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/FYP_train.csv\")\n",
    "\n",
    "data = data[[\"text\", \"target\"]]\n",
    "data.rename(columns = {\"text\" : \"Message\", \"target\" : \"Label\"}, inplace = True)\n",
    "\n",
    "print(\"Number of rows in data:\", data.shape[0])\n",
    "print(\"Number of columns in data:\", data.shape[1])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Message\"].apply(processText)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./dataset/FYP_train_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    encoding=\"utf-8\", \n",
    "    strip_accents=\"unicode\", \n",
    "    stop_words=\"english\", \n",
    "    lowercase=True, \n",
    "    max_features=500\n",
    ")\n",
    "tfidf_result = vectorizer.fit_transform(data[\"Message\"])\n",
    "tfidf_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open('./models/message_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FYP_train_X_TFIDF = tfidf_result.toarray()\n",
    "FYP_train_Y = data[\"Label\"].values\n",
    "\n",
    "np.save(\"./dataset/FYP_train_X_TFIDF.npy\", FYP_train_X_TFIDF)\n",
    "np.save(\"./dataset/FYP_train_Y.npy\", FYP_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "glove = open(\"./dataset/glove.6B.50d.txt\", encoding=\"utf8\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        C = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        pass\n",
    "    embeddings[word] = C  \n",
    "glove.close()\n",
    "pickle.dump(embeddings, open('./dataset/glove_embeddings.pkl', 'wb'))\n",
    "print(f'{len(embeddings)} Word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=75000)\n",
    "tokenizer.fit_on_texts(data[\"Message\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Message\"])\n",
    "paddedSequence = pad_sequences(sequences, maxlen=500)\n",
    "wordIndex = tokenizer.word_index\n",
    "print(f'{len(wordIndex)} Unique tokens')\n",
    "pickle.dump(tokenizer, open('./models/message_tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FYP_train_X_SEQ = paddedSequence\n",
    "\n",
    "np.save(\"./dataset/FYP_train_X_SEQ.npy\", FYP_train_X_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FYP_train_X_SEQ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FYP_train_X_TFIDF.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30d15984fd22aa96de85f16433eec91bbe2faea5d46ed3d4d24713e4f4ec970c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
